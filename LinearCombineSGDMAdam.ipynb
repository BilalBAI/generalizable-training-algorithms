{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_83TyHzntNuv",
        "outputId": "b1c45c74-898c-4ce4-a22f-9659f673fcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Tue Mar  7 01:44:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    24W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@title Connections\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define path\n",
        "path='/content/drive/MyDrive/Colab Notebooks/PADAM/'\n",
        "\n",
        "# Check GPU Info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf8m1T47Us-4",
        "outputId": "83fdc96b-5caf-4651-af87-a9ad170af0f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting adabound\n",
            "  Downloading adabound-0.0.5-py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from adabound) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.0->adabound) (4.5.0)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ]
        }
      ],
      "source": [
        "#@title imports\n",
        "!pip install adabound\n",
        "#!pip install merlin-dataloader\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import adabound\n",
        "\n",
        "import json\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GYnlEQEaV296"
      },
      "outputs": [],
      "source": [
        "#@title utils\n",
        "'''Some helper functions for PyTorch, including:\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
        "    - msr_init: net parameter initialization.\n",
        "    - progress_bar: progress bar mimic xlua.progress.\n",
        "'''\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "# term_width = int(term_width)\n",
        "term_width=20\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(str(msg))\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wKeN_HDbVTnJ"
      },
      "outputs": [],
      "source": [
        "#@title VGG\n",
        "'''VGG11/13/16/19 in Pytorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name, num_classes = 10):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "# net = VGG('VGG11')\n",
        "# x = torch.randn(2,3,32,32)\n",
        "# print(net(Variable(x)).size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O0cfG8IxUxGK"
      },
      "outputs": [],
      "source": [
        "#@title LinearCombineSGDMAdam\n",
        "from torch.optim import *\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class LinearCombineSGDMAdam(Optimizer):\n",
        "    \"\"\"Implements Partially adaptive momentum estimation (Padam) algorithm.\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-1)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        partial (float, optional): partially adaptive parameter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-1, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=True, partial = 1/2, a=1):\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, partial = partial,a=a)\n",
        "        super(LinearCombineSGDMAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None, epoch=None, target_epoch=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "                partial = group['partial']\n",
        "                a = group['a']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(p.data, alpha=group['weight_decay'])\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(grad,alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                else:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                if epoch is not None and target_epoch is not None:\n",
        "                    adjusted_a = a + min(epoch/target_epoch,1) * (1-a)\n",
        "                else:\n",
        "                    adjusted_a = a\n",
        "\n",
        "                p.data.addcmul_(exp_avg, adjusted_a + (1-adjusted_a)*denom**(-partial*2), value=-step_size )\n",
        "                \n",
        "        return loss\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0hJcfBT2kN91"
      },
      "outputs": [],
      "source": [
        "#@title train_loop( ) and test_loop( )\n",
        "def train_loop(trainloader, model, criterion, optimizer, use_cuda, train_errs, train_losses,epoch=None,target_epoch=None):\n",
        "    # Training\n",
        "    model.train()  \n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        def closure():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            return loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        if epoch is None and target_epoch is None:\n",
        "            optimizer.step(closure)\n",
        "        else:\n",
        "            optimizer.step(closure,epoch,target_epoch)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                      % (train_loss/(batch_idx+1), 100.0/total*(correct), correct, total))\n",
        "\n",
        "    # Compute training error\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                      % (train_loss/(batch_idx+1), 100.0/total*(correct), correct, total))\n",
        "    train_errs.append(1 - correct/total)\n",
        "    train_losses.append(train_loss/(batch_idx+1))\n",
        "\n",
        "\n",
        "def test_loop(testloader, model, criterion, use_cuda, test_errs, test_losses):\n",
        "    # Testing\n",
        "    model.eval()  \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                      % (test_loss/(batch_idx+1), 100.0 / total * (correct), correct, total))\n",
        "    test_errs.append(1 - correct/total)\n",
        "    test_losses.append(test_loss/(batch_idx+1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VnGEOE4jH1Ka"
      },
      "outputs": [],
      "source": [
        "#@title Data Loader\n",
        "\n",
        "def load_cifar10():\n",
        "    print('==> Preparing cifar10 data..')\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                              (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                              (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=f'{path}/data/', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=f'{path}/data/', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "                'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "def load_cifar100():\n",
        "    print('==> Preparing cifar100 data..')\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276]),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root=path+'/data/CIFAR100', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root=path+'/data/CIFAR100', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    return trainloader, testloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yvbPZRAyLs48"
      },
      "outputs": [],
      "source": [
        "#@title def LinearCombineSGDMAdam_switch(switch,lr,lr_sgdm,dataset,a=0.7)\n",
        "\n",
        "def LinearCombineSGDMAdam_switch(switch,lr,lr_sgdm,dataset,a=0.7):\n",
        "\n",
        "    # Hyperparameters ##############################################\n",
        "\n",
        "    # Tuning Hyperparameters\n",
        "    # switch=100 # switch to SGDM\n",
        "    # lr=0.001 # lr for LinearCombineAdamSGDM\n",
        "    # lr_sgdm=0.007\n",
        "    # a=0.7 # LinearCombineAdamSGDM weight for SGDM.  weight for Adam is 1-a\n",
        "\n",
        "    # Fixed Hyperparameters\n",
        "    net=\"vggnet\"\n",
        "    method=f\"LinearCombineSGDMAdam_a={a}_lr={lr}_switch={switch}_lr_sgdm={lr_sgdm}\"\n",
        "    wd = 5e-4\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    Nepoch=200\n",
        "    resume=True\n",
        "\n",
        "    print(method)\n",
        "\n",
        "    # init #########################################################\n",
        "    results_path=f'{path}/checkpoint/{net}/{dataset}/{method}'\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    best_acc = 0  # best test accuracy\n",
        "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "    train_errs = []\n",
        "    test_errs = []\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    lrs = []\n",
        "\n",
        "    # Data #########################################################\n",
        "    if dataset == 'CIFAR10':\n",
        "        trainloader, testloader = load_cifar10()\n",
        "    elif dataset == 'CIFAR100':\n",
        "        trainloader, testloader = load_cifar100()\n",
        "    else:\n",
        "        print(f'{dataset} Not Available!')\n",
        "        return 0\n",
        "    # Model #########################################################\n",
        "    if resume:\n",
        "        # Load checkpoint.\n",
        "        print('==> Resuming from checkpoint..')\n",
        "        try:\n",
        "            checkpoint = torch.load(results_path)\n",
        "            model = checkpoint['model']\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            train_losses = checkpoint['train_losses']\n",
        "            train_errs = checkpoint['train_errs']\n",
        "            test_losses = checkpoint['test_losses']\n",
        "            test_errs = checkpoint['test_errs']\n",
        "            best_acc = checkpoint['best_acc']\n",
        "            lrs = checkpoint['lrs']\n",
        "            lr=lrs[-1]\n",
        "            if start_epoch > switch:\n",
        "              lr_sgdm = lr\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print('==> Building model..')\n",
        "            if net == 'vggnet':\n",
        "                if dataset == 'CIFAR10':\n",
        "                    model = VGG('VGG16', num_classes=10)\n",
        "                elif dataset == 'CIFAR100':\n",
        "                    model = VGG('VGG16', num_classes = 100)\n",
        "                else:\n",
        "                    print(f'{dataset} model Not Available!')\n",
        "                    return 0\n",
        "            # #     model = models.vgg16_bn(num_classes=10)\n",
        "            # elif net == 'resnet':\n",
        "            #     model = ResNet18(num_classes=10)\n",
        "            # #     model = models.resnet18(num_classes=10)\n",
        "            # elif net == 'wideresnet':\n",
        "            #     model = WResNet_cifar10(\n",
        "            #         num_classes=10, depth=16, multiplier=4)\n",
        "            else:\n",
        "                print('Network undefined!')\n",
        "    else:\n",
        "        print('==> Building model..')\n",
        "        if net == 'vggnet':\n",
        "            if dataset == 'CIFAR10':\n",
        "                model = VGG('VGG16', num_classes=10)\n",
        "            elif dataset == 'CIFAR100':\n",
        "                model = VGG('VGG16', num_classes = 100)\n",
        "            else:\n",
        "                print(f'{dataset} model Not Available!')\n",
        "                return 0\n",
        "        # #     model = models.vgg16_bn(num_classes=10)\n",
        "        # elif net == 'resnet':\n",
        "        #     model = ResNet18(num_classes=10)\n",
        "        # #     model = models.resnet18(num_classes=10)\n",
        "        # elif net == 'wideresnet':\n",
        "        #     model = WResNet_cifar10(\n",
        "        #         num_classes=10, depth=16, multiplier=4)\n",
        "        else:\n",
        "            print('Network undefined!')\n",
        "\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "        model = torch.nn.DataParallel(\n",
        "            model, device_ids=range(torch.cuda.device_count()))\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "\n",
        "    # Optimize/Train #####################################\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    betas = (beta1, beta2)\n",
        "    optimizer = LinearCombineSGDMAdam(model.parameters(), lr=lr, weight_decay = wd, betas = betas, a=a)\n",
        "    milestones=[100-start_epoch,150-start_epoch]\n",
        "    # milestones=[150-start_epoch]\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[i for i in milestones if i>0], gamma=0.1)\n",
        "\n",
        "    for epoch in range(start_epoch+1, switch+1):\n",
        "        print('\\nEpoch: %d' % epoch, ' Learning rate:', scheduler.get_last_lr(), ' best_acc: ', best_acc)\n",
        "        # stop training if best_acc is significantly inferior from the 20's epoch\n",
        "        if epoch > 20 and best_acc < 80:\n",
        "          print('inferior')\n",
        "          break\n",
        "        # Train\n",
        "        train_loop(trainloader=trainloader, model=model, \n",
        "                    criterion=criterion, optimizer=optimizer,\n",
        "                    use_cuda=use_cuda,train_errs=train_errs,train_losses=train_losses)\n",
        "        # Test\n",
        "        test_loop(testloader=testloader, model=model, criterion=criterion, \n",
        "                  use_cuda=use_cuda, test_errs=test_errs, test_losses=test_losses)\n",
        "        # Save checkpoint\n",
        "        acc = 100.0 * (1-test_errs[-1])\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "        lrs.extend(scheduler.get_last_lr())\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'model': model,\n",
        "            'epoch': epoch,\n",
        "            'best_acc': best_acc,\n",
        "            'train_errs': train_errs,\n",
        "            'train_losses': train_losses,\n",
        "            'test_errs': test_errs,\n",
        "            'test_losses': test_losses,\n",
        "            'lrs': lrs\n",
        "        }\n",
        "        torch.save(state, results_path)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    if lr_sgdm == 'NA':\n",
        "      lr_sgdm = 0\n",
        "    if lr_sgdm > 0:\n",
        "        # Switch to SGDM\n",
        "        print('============ Switch to sgdm ==========')\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr_sgdm, momentum= 0.9, weight_decay = wd)\n",
        "        milestones=[100-max(switch,start_epoch),150-max(switch,start_epoch)]\n",
        "        # milestones=[150-max(switch,start_epoch)]\n",
        "        print([i for i in milestones if i>0])\n",
        "        scheduler = MultiStepLR(optimizer, milestones=[i for i in milestones if i>0], gamma=0.1)\n",
        "\n",
        "        for epoch in range(max(switch+1,start_epoch+1), 200+1):\n",
        "            print('\\nEpoch: %d' % epoch, ' Learning rate:', scheduler.get_last_lr(), ' best_acc: ', best_acc)\n",
        "            # Train\n",
        "            train_loop(trainloader=trainloader, model=model, \n",
        "                        criterion=criterion, optimizer=optimizer,\n",
        "                        use_cuda=use_cuda,train_errs=train_errs,train_losses=train_losses)\n",
        "            # Test\n",
        "            test_loop(testloader=testloader, model=model, criterion=criterion, \n",
        "                      use_cuda=use_cuda, test_errs=test_errs, test_losses=test_losses)\n",
        "            # Save checkpoint\n",
        "            acc = 100.0 * (1-test_errs[-1])\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "            lrs.extend(scheduler.get_last_lr())\n",
        "            print('Saving..')\n",
        "            state = {\n",
        "                'model': model,\n",
        "                'epoch': epoch,\n",
        "                'best_acc': best_acc,\n",
        "                'train_errs': train_errs,\n",
        "                'train_losses': train_losses,\n",
        "                'test_errs': test_errs,\n",
        "                'test_losses': test_losses,\n",
        "                'lrs': lrs\n",
        "            }\n",
        "            torch.save(state, results_path)\n",
        "\n",
        "            scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UozRhxwbKY6m"
      },
      "outputs": [],
      "source": [
        "#@title def LinearCombineSGDMAdam_dynamic_weighting(target_epoch,lr,lr_sgdm,dataset,a=0.7)\n",
        "\n",
        "def LinearCombineSGDMAdam_dynamic_weighting(target_epoch,lr,lr_sgdm,dataset,a=0.7):\n",
        "\n",
        "    # Hyperparameters ##############################################\n",
        "    # Tuning Hyperparameters\n",
        "    # target_epoch=50 # target_epoch for a to meet 1 (i.e. switch to SGDM).\n",
        "    # lr=0.05 # lr for LinearCombineAdamSGDM\n",
        "    # a=0.7 # LinearCombineAdamSGDM weight for SGDM.  weight for Adam is 1-a\n",
        "\n",
        "    # Fixed Hyperparameters\n",
        "    net=\"vggnet\"\n",
        "    method=f\"LinearCombineSGDMAdam_a={a}_lr={lr}_lr_sgdm={lr_sgdm}_target_epoch={target_epoch}\"\n",
        "    wd = 5e-4\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    Nepoch=200\n",
        "    resume=True\n",
        "\n",
        "    print(method)\n",
        "\n",
        "    # init #########################################################\n",
        "    results_path=f'{path}/checkpoint/{net}/{dataset}/{method}'\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    best_acc = 0  # best test accuracy\n",
        "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "    train_errs = []\n",
        "    test_errs = []\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    lrs = []\n",
        "\n",
        "    # Data #########################################################\n",
        "    if dataset == 'CIFAR10':\n",
        "        trainloader, testloader = load_cifar10()\n",
        "    elif dataset == 'CIFAR100':\n",
        "        trainloader, testloader = load_cifar100()\n",
        "    else:\n",
        "        print(f'{dataset} Not Available!')\n",
        "        return 0\n",
        "\n",
        "    # Model #########################################################\n",
        "    if resume:\n",
        "        # Load checkpoint.\n",
        "        print('==> Resuming from checkpoint..')\n",
        "        try:\n",
        "            checkpoint = torch.load(results_path)\n",
        "            model = checkpoint['model']\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            train_losses = checkpoint['train_losses']\n",
        "            train_errs = checkpoint['train_errs']\n",
        "            test_losses = checkpoint['test_losses']\n",
        "            test_errs = checkpoint['test_errs']\n",
        "            best_acc = checkpoint['best_acc']\n",
        "            lrs = checkpoint['lrs']\n",
        "            lr=lrs[-1]\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print('==> Building model..')\n",
        "            if net == 'vggnet':\n",
        "                if dataset == 'CIFAR10':\n",
        "                    model = VGG('VGG16', num_classes=10)\n",
        "                elif dataset == 'CIFAR100':\n",
        "                    model = VGG('VGG16', num_classes = 100)\n",
        "                else:\n",
        "                    print(f'{dataset} model Not Available!')\n",
        "                    return 0\n",
        "            # #     model = models.vgg16_bn(num_classes=10)\n",
        "            # elif net == 'resnet':\n",
        "            #     model = ResNet18(num_classes=10)\n",
        "            # #     model = models.resnet18(num_classes=10)\n",
        "            # elif net == 'wideresnet':\n",
        "            #     model = WResNet_cifar10(\n",
        "            #         num_classes=10, depth=16, multiplier=4)\n",
        "            else:\n",
        "                print('Network undefined!')\n",
        "    else:\n",
        "        print('==> Building model..')\n",
        "        if net == 'vggnet':\n",
        "            if dataset == 'CIFAR10':\n",
        "                model = VGG('VGG16', num_classes=10)\n",
        "            elif dataset == 'CIFAR100':\n",
        "                model = VGG('VGG16', num_classes = 100)\n",
        "            else:\n",
        "                print(f'{dataset} model Not Available!')\n",
        "                return 0\n",
        "        # #     model = models.vgg16_bn(num_classes=10)\n",
        "        # elif net == 'resnet':\n",
        "        #     model = ResNet18(num_classes=10)\n",
        "        # #     model = models.resnet18(num_classes=10)\n",
        "        # elif net == 'wideresnet':\n",
        "        #     model = WResNet_cifar10(\n",
        "        #         num_classes=10, depth=16, multiplier=4)\n",
        "        else:\n",
        "            print('Network undefined!')\n",
        "\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "        model = torch.nn.DataParallel(\n",
        "            model, device_ids=range(torch.cuda.device_count()))\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "    # Optimize/Train #####################################\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    betas = (beta1, beta2)\n",
        "    optimizer = LinearCombineSGDMAdam(model.parameters(), lr=lr, weight_decay = wd, betas = betas, a=a)\n",
        "    milestones=[100-start_epoch,150-start_epoch]\n",
        "    print([i for i in milestones if i>0])\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[i for i in milestones if i>0], gamma=0.1)\n",
        "\n",
        "    for epoch in range(start_epoch+1, Nepoch+1):\n",
        "        print('\\nEpoch: %d' % epoch, ' Learning rate:', scheduler.get_last_lr(), ' best_acc: ', best_acc, ' a:',a + min(epoch/target_epoch,1) * (1-a))\n",
        "        if epoch == target_epoch+1:\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = lr_sgdm\n",
        "        # stop training if best_acc is significantly inferior from the 20's epoch\n",
        "        if epoch > 20 and best_acc < 80:\n",
        "          print('inferior')\n",
        "          break\n",
        "        # Train\n",
        "        train_loop(trainloader=trainloader, model=model, \n",
        "                    criterion=criterion, optimizer=optimizer,\n",
        "                    use_cuda=use_cuda,train_errs=train_errs,train_losses=train_losses,epoch=epoch,target_epoch=target_epoch)\n",
        "        # Test\n",
        "        test_loop(testloader=testloader, model=model, criterion=criterion, \n",
        "                  use_cuda=use_cuda, test_errs=test_errs, test_losses=test_losses)\n",
        "        # Save checkpoint\n",
        "        acc = 100.0 * (1-test_errs[-1])\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "        lrs.extend(scheduler.get_last_lr())\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'model': model,\n",
        "            'epoch': epoch,\n",
        "            'best_acc': best_acc,\n",
        "            'train_errs': train_errs,\n",
        "            'train_losses': train_losses,\n",
        "            'test_errs': test_errs,\n",
        "            'test_losses': test_losses,\n",
        "            'lrs': lrs\n",
        "        }\n",
        "        torch.save(state, results_path)\n",
        "\n",
        "        scheduler.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BOtJZfxK0YK"
      },
      "outputs": [],
      "source": [
        "# for switch in [200]:\n",
        "#   for a in [0.9,0.7,0.5,0.3,0.1]:\n",
        "#     for lr in [0.001,0.003,0.005,0.007,0.009]:\n",
        "#       LinearCombineSGDMAdam_switch(switch=switch,lr=lr,lr_sgdm='NA',dataset='CIFAR10',a=a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for target_epoch in [50]:\n",
        "#   for a in [0.9,0.7,0.5,0.3,0.1]:\n",
        "#     for lr in [0.001,0.005,0.01,0.05,0.09]:\n",
        "#       LinearCombineSGDMAdam_dynamic_weighting(target_epoch=target_epoch,lr=lr,lr_sgdm=lr, dataset='CIFAR10',a=a)"
      ],
      "metadata": {
        "id": "QUWnSGwNyRVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for target_epoch in [50]:\n",
        "  for a in [0.9,0.7,0.5,0.3,0.1]:\n",
        "    for lr in [0.001,0.005,0.01,0.05,0.09]:\n",
        "      LinearCombineSGDMAdam_dynamic_weighting(target_epoch=target_epoch,lr=lr,lr_sgdm=lr*1.5, dataset='CIFAR10',a=a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcPZOITpQfMG",
        "outputId": "8b36220f-b0ea-4606-eadf-0fd0e3522859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearCombineSGDMAdam_a=0.9_lr=0.001_lr_sgdm=0.0015_target_epoch=50\n",
            "==> Preparing cifar10 data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Resuming from checkpoint..\n",
            "[]\n",
            "LinearCombineSGDMAdam_a=0.9_lr=0.005_lr_sgdm=0.0075_target_epoch=50\n",
            "==> Preparing cifar10 data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Resuming from checkpoint..\n",
            "[]\n",
            "LinearCombineSGDMAdam_a=0.9_lr=0.01_lr_sgdm=0.015_target_epoch=50\n",
            "==> Preparing cifar10 data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Resuming from checkpoint..\n",
            "[]\n",
            "\n",
            "Epoch: 190  Learning rate: [0.00015000000000000001]  best_acc:  92.78  a: 1.0\n",
            " [================================================================>]  Step: 606ms | Tot: 31s799ms | Loss: 0.003 | Acc: 99.936% (49968/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s970ms | Loss: 0.003 | Acc: 99.956% (49978/50000) 391/391 \n",
            " [================================================================>]  Step: 33ms | Tot: 3s186ms | Loss: 0.365 | Acc: 92.650% (9265/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 191  Learning rate: [0.00015000000000000001]  best_acc:  92.78  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s613ms | Loss: 0.003 | Acc: 99.960% (49980/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s298ms | Loss: 0.003 | Acc: 99.942% (49971/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 3s21ms | Loss: 0.367 | Acc: 92.550% (9255/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 192  Learning rate: [0.00015000000000000001]  best_acc:  92.78  a: 1.0\n",
            " [================================================================>]  Step: 71ms | Tot: 31s589ms | Loss: 0.003 | Acc: 99.946% (49973/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s902ms | Loss: 0.003 | Acc: 99.956% (49978/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 3s97ms | Loss: 0.364 | Acc: 92.640% (9264/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 193  Learning rate: [0.00015000000000000001]  best_acc:  92.78  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s651ms | Loss: 0.003 | Acc: 99.950% (49975/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 15s732ms | Loss: 0.003 | Acc: 99.936% (49968/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s784ms | Loss: 0.363 | Acc: 92.650% (9265/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 194  Learning rate: [0.00015000000000000001]  best_acc:  92.78  a: 1.0\n",
            " [================================================================>]  Step: 63ms | Tot: 31s795ms | Loss: 0.003 | Acc: 99.920% (49960/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 15s828ms | Loss: 0.003 | Acc: 99.934% (49967/50000) 391/391 \n",
            " [================================================================>]  Step: 18ms | Tot: 2s815ms | Loss: 0.365 | Acc: 92.630% (9263/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 195  Learning rate: [0.00015000000000000001]  best_acc:  92.78  a: 1.0\n",
            " [================================================================>]  Step: 66ms | Tot: 31s892ms | Loss: 0.003 | Acc: 99.928% (49964/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s446ms | Loss: 0.003 | Acc: 99.924% (49962/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s862ms | Loss: 0.362 | Acc: 92.830% (9283/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 196  Learning rate: [0.00015000000000000001]  best_acc:  92.83  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s899ms | Loss: 0.003 | Acc: 99.930% (49965/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 15s933ms | Loss: 0.003 | Acc: 99.938% (49969/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s846ms | Loss: 0.362 | Acc: 92.680% (9268/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 197  Learning rate: [0.00015000000000000001]  best_acc:  92.83  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s894ms | Loss: 0.003 | Acc: 99.936% (49968/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s937ms | Loss: 0.003 | Acc: 99.946% (49973/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s853ms | Loss: 0.363 | Acc: 92.580% (9258/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 198  Learning rate: [0.00015000000000000001]  best_acc:  92.83  a: 1.0\n",
            " [================================================================>]  Step: 67ms | Tot: 32s10ms | Loss: 0.003 | Acc: 99.950% (49975/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s753ms | Loss: 0.003 | Acc: 99.942% (49971/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s821ms | Loss: 0.366 | Acc: 92.640% (9264/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 199  Learning rate: [0.00015000000000000001]  best_acc:  92.83  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s891ms | Loss: 0.003 | Acc: 99.926% (49963/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 16s367ms | Loss: 0.003 | Acc: 99.958% (49979/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s822ms | Loss: 0.363 | Acc: 92.650% (9265/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 200  Learning rate: [0.00015000000000000001]  best_acc:  92.83  a: 1.0\n",
            " [================================================================>]  Step: 65ms | Tot: 31s942ms | Loss: 0.003 | Acc: 99.938% (49969/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 15s740ms | Loss: 0.003 | Acc: 99.942% (49971/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s781ms | Loss: 0.364 | Acc: 92.620% (9262/10000) 100/100 \n",
            "Saving..\n",
            "LinearCombineSGDMAdam_a=0.9_lr=0.05_lr_sgdm=0.07500000000000001_target_epoch=50\n",
            "==> Preparing cifar10 data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Resuming from checkpoint..\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/PADAM//checkpoint/vggnet/CIFAR10/LinearCombineSGDMAdam_a=0.9_lr=0.05_lr_sgdm=0.07500000000000001_target_epoch=50'\n",
            "==> Building model..\n",
            "[100, 150]\n",
            "\n",
            "Epoch: 1  Learning rate: [0.05]  best_acc:  0  a: 0.902\n",
            " [================================================================>]  Step: 62ms | Tot: 31s816ms | Loss: 2.043 | Acc: 21.170% (10585/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 15s945ms | Loss: 1.740 | Acc: 32.418% (16209/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s749ms | Loss: 1.751 | Acc: 31.460% (3146/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 2  Learning rate: [0.05]  best_acc:  31.46  a: 0.904\n",
            " [================================================================>]  Step: 64ms | Tot: 31s816ms | Loss: 1.576 | Acc: 38.102% (19051/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 15s665ms | Loss: 1.442 | Acc: 45.214% (22607/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s683ms | Loss: 1.420 | Acc: 45.900% (4590/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 3  Learning rate: [0.05]  best_acc:  45.900000000000006  a: 0.906\n",
            " [================================================================>]  Step: 63ms | Tot: 31s506ms | Loss: 1.303 | Acc: 51.740% (25870/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 15s994ms | Loss: 1.146 | Acc: 59.330% (29665/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s991ms | Loss: 1.124 | Acc: 59.830% (5983/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 4  Learning rate: [0.05]  best_acc:  59.830000000000005  a: 0.908\n",
            " [================================================================>]  Step: 64ms | Tot: 31s319ms | Loss: 1.061 | Acc: 62.160% (31080/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 15s837ms | Loss: 0.971 | Acc: 64.786% (32393/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s739ms | Loss: 0.967 | Acc: 65.360% (6536/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 5  Learning rate: [0.05]  best_acc:  65.36  a: 0.91\n",
            " [================================================================>]  Step: 61ms | Tot: 31s326ms | Loss: 0.942 | Acc: 66.576% (33288/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s985ms | Loss: 0.895 | Acc: 68.410% (34205/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s797ms | Loss: 0.895 | Acc: 68.160% (6816/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 6  Learning rate: [0.05]  best_acc:  68.16  a: 0.912\n",
            " [================================================================>]  Step: 60ms | Tot: 31s267ms | Loss: 0.861 | Acc: 70.152% (35076/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s543ms | Loss: 0.804 | Acc: 72.278% (36139/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s928ms | Loss: 0.784 | Acc: 72.870% (7287/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 7  Learning rate: [0.05]  best_acc:  72.87  a: 0.914\n",
            " [================================================================>]  Step: 63ms | Tot: 31s244ms | Loss: 0.801 | Acc: 72.486% (36243/50000) 391/391 \n",
            " [================================================================>]  Step: 15ms | Tot: 15s580ms | Loss: 0.749 | Acc: 74.330% (37165/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s731ms | Loss: 0.746 | Acc: 74.530% (7453/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 8  Learning rate: [0.05]  best_acc:  74.53  a: 0.916\n",
            " [================================================================>]  Step: 56ms | Tot: 31s488ms | Loss: 0.765 | Acc: 74.076% (37038/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s865ms | Loss: 0.700 | Acc: 76.310% (38155/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s744ms | Loss: 0.713 | Acc: 76.050% (7605/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 9  Learning rate: [0.05]  best_acc:  76.05  a: 0.918\n",
            " [================================================================>]  Step: 60ms | Tot: 31s449ms | Loss: 0.738 | Acc: 75.080% (37540/50000) 391/391 \n",
            " [================================================================>]  Step: 18ms | Tot: 15s752ms | Loss: 0.696 | Acc: 76.314% (38157/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s660ms | Loss: 0.678 | Acc: 76.700% (7670/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 10  Learning rate: [0.05]  best_acc:  76.7  a: 0.92\n",
            " [================================================================>]  Step: 65ms | Tot: 31s294ms | Loss: 0.706 | Acc: 76.094% (38047/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 15s362ms | Loss: 0.673 | Acc: 77.430% (38715/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s754ms | Loss: 0.690 | Acc: 76.760% (7676/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 11  Learning rate: [0.05]  best_acc:  76.75999999999999  a: 0.922\n",
            " [================================================================>]  Step: 63ms | Tot: 31s298ms | Loss: 0.685 | Acc: 77.018% (38509/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s833ms | Loss: 0.637 | Acc: 78.682% (39341/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s701ms | Loss: 0.656 | Acc: 77.680% (7768/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 12  Learning rate: [0.05]  best_acc:  77.68  a: 0.924\n",
            " [================================================================>]  Step: 58ms | Tot: 31s383ms | Loss: 0.663 | Acc: 77.630% (38815/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s617ms | Loss: 0.661 | Acc: 77.604% (38802/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s707ms | Loss: 0.686 | Acc: 76.820% (7682/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 13  Learning rate: [0.05]  best_acc:  77.68  a: 0.926\n",
            " [================================================================>]  Step: 60ms | Tot: 31s344ms | Loss: 0.650 | Acc: 78.378% (39189/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 16s86ms | Loss: 0.639 | Acc: 78.750% (39375/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s850ms | Loss: 0.653 | Acc: 78.170% (7817/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 14  Learning rate: [0.05]  best_acc:  78.17  a: 0.928\n",
            " [================================================================>]  Step: 69ms | Tot: 31s280ms | Loss: 0.639 | Acc: 78.530% (39265/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 15s996ms | Loss: 0.619 | Acc: 79.092% (39546/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s912ms | Loss: 0.632 | Acc: 78.750% (7875/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 15  Learning rate: [0.05]  best_acc:  78.75  a: 0.93\n",
            " [================================================================>]  Step: 60ms | Tot: 31s323ms | Loss: 0.619 | Acc: 79.268% (39634/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 15s973ms | Loss: 0.606 | Acc: 79.678% (39839/50000) 391/391 \n",
            " [================================================================>]  Step: 30ms | Tot: 3s6ms | Loss: 0.628 | Acc: 78.760% (7876/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 16  Learning rate: [0.05]  best_acc:  78.75999999999999  a: 0.932\n",
            " [================================================================>]  Step: 66ms | Tot: 31s473ms | Loss: 0.608 | Acc: 79.748% (39874/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 15s644ms | Loss: 0.582 | Acc: 80.760% (40380/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s839ms | Loss: 0.615 | Acc: 79.460% (7946/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 17  Learning rate: [0.05]  best_acc:  79.46  a: 0.934\n",
            " [================================================================>]  Step: 62ms | Tot: 31s370ms | Loss: 0.598 | Acc: 80.012% (40006/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 15s647ms | Loss: 0.567 | Acc: 81.172% (40586/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s729ms | Loss: 0.587 | Acc: 80.550% (8055/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 18  Learning rate: [0.05]  best_acc:  80.55  a: 0.936\n",
            " [================================================================>]  Step: 59ms | Tot: 31s440ms | Loss: 0.588 | Acc: 80.318% (40159/50000) 391/391 \n",
            " [================================================================>]  Step: 30ms | Tot: 15s686ms | Loss: 0.561 | Acc: 81.114% (40557/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s714ms | Loss: 0.602 | Acc: 79.890% (7989/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 19  Learning rate: [0.05]  best_acc:  80.55  a: 0.9380000000000001\n",
            " [================================================================>]  Step: 63ms | Tot: 31s427ms | Loss: 0.574 | Acc: 80.884% (40442/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 15s799ms | Loss: 0.569 | Acc: 80.860% (40430/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s764ms | Loss: 0.595 | Acc: 80.240% (8024/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 20  Learning rate: [0.05]  best_acc:  80.55  a: 0.9400000000000001\n",
            " [================================================================>]  Step: 64ms | Tot: 31s533ms | Loss: 0.568 | Acc: 81.062% (40531/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 15s788ms | Loss: 0.517 | Acc: 82.912% (41456/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s727ms | Loss: 0.559 | Acc: 81.190% (8119/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 21  Learning rate: [0.05]  best_acc:  81.19  a: 0.9420000000000001\n",
            " [================================================================>]  Step: 59ms | Tot: 31s318ms | Loss: 0.561 | Acc: 81.290% (40645/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 16s52ms | Loss: 0.552 | Acc: 81.824% (40912/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s699ms | Loss: 0.574 | Acc: 81.190% (8119/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 22  Learning rate: [0.05]  best_acc:  81.19  a: 0.9440000000000001\n",
            " [================================================================>]  Step: 60ms | Tot: 31s218ms | Loss: 0.545 | Acc: 81.690% (40845/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 16s242ms | Loss: 0.494 | Acc: 83.576% (41788/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s815ms | Loss: 0.532 | Acc: 82.580% (8258/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 23  Learning rate: [0.05]  best_acc:  82.58  a: 0.9460000000000001\n",
            " [================================================================>]  Step: 64ms | Tot: 31s259ms | Loss: 0.540 | Acc: 81.982% (40991/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 15s417ms | Loss: 0.510 | Acc: 83.102% (41551/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 3s61ms | Loss: 0.543 | Acc: 82.110% (8211/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 24  Learning rate: [0.05]  best_acc:  82.58  a: 0.948\n",
            " [================================================================>]  Step: 54ms | Tot: 31s496ms | Loss: 0.526 | Acc: 82.684% (41342/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s516ms | Loss: 0.544 | Acc: 81.958% (40979/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 2s766ms | Loss: 0.581 | Acc: 80.960% (8096/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 25  Learning rate: [0.05]  best_acc:  82.58  a: 0.95\n",
            " [================================================================>]  Step: 60ms | Tot: 31s349ms | Loss: 0.523 | Acc: 82.554% (41277/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s657ms | Loss: 0.501 | Acc: 83.572% (41786/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s669ms | Loss: 0.533 | Acc: 82.540% (8254/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 26  Learning rate: [0.05]  best_acc:  82.58  a: 0.952\n",
            " [================================================================>]  Step: 61ms | Tot: 31s349ms | Loss: 0.509 | Acc: 83.086% (41543/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 15s519ms | Loss: 0.498 | Acc: 83.274% (41637/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s668ms | Loss: 0.551 | Acc: 82.020% (8202/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 27  Learning rate: [0.05]  best_acc:  82.58  a: 0.954\n",
            " [================================================================>]  Step: 68ms | Tot: 31s271ms | Loss: 0.508 | Acc: 83.078% (41539/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s171ms | Loss: 0.486 | Acc: 83.662% (41831/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s708ms | Loss: 0.523 | Acc: 82.590% (8259/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 28  Learning rate: [0.05]  best_acc:  82.59  a: 0.956\n",
            " [================================================================>]  Step: 61ms | Tot: 31s296ms | Loss: 0.495 | Acc: 83.564% (41782/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s708ms | Loss: 0.482 | Acc: 83.832% (41916/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s777ms | Loss: 0.543 | Acc: 81.880% (8188/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 29  Learning rate: [0.05]  best_acc:  82.59  a: 0.958\n",
            " [================================================================>]  Step: 64ms | Tot: 31s321ms | Loss: 0.478 | Acc: 84.084% (42042/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 15s820ms | Loss: 0.476 | Acc: 83.948% (41974/50000) 391/391 \n",
            " [================================================================>]  Step: 31ms | Tot: 2s749ms | Loss: 0.512 | Acc: 83.180% (8318/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 30  Learning rate: [0.05]  best_acc:  83.17999999999999  a: 0.96\n",
            " [================================================================>]  Step: 63ms | Tot: 31s241ms | Loss: 0.478 | Acc: 84.128% (42064/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 16s295ms | Loss: 0.453 | Acc: 84.778% (42389/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s830ms | Loss: 0.488 | Acc: 83.590% (8359/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 31  Learning rate: [0.05]  best_acc:  83.59  a: 0.962\n",
            " [================================================================>]  Step: 61ms | Tot: 31s301ms | Loss: 0.465 | Acc: 84.666% (42333/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 16s908ms | Loss: 0.442 | Acc: 85.304% (42652/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s943ms | Loss: 0.491 | Acc: 83.790% (8379/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 32  Learning rate: [0.05]  best_acc:  83.78999999999999  a: 0.964\n",
            " [================================================================>]  Step: 57ms | Tot: 31s430ms | Loss: 0.459 | Acc: 84.688% (42344/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 16s618ms | Loss: 0.425 | Acc: 85.810% (42905/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s960ms | Loss: 0.476 | Acc: 84.060% (8406/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 33  Learning rate: [0.05]  best_acc:  84.06  a: 0.966\n",
            " [================================================================>]  Step: 66ms | Tot: 31s417ms | Loss: 0.446 | Acc: 85.244% (42622/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 16s307ms | Loss: 0.411 | Acc: 86.322% (43161/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 3s95ms | Loss: 0.477 | Acc: 84.350% (8435/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 34  Learning rate: [0.05]  best_acc:  84.35000000000001  a: 0.968\n",
            " [================================================================>]  Step: 66ms | Tot: 31s383ms | Loss: 0.442 | Acc: 85.248% (42624/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s89ms | Loss: 0.408 | Acc: 86.414% (43207/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 2s913ms | Loss: 0.464 | Acc: 85.160% (8516/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 35  Learning rate: [0.05]  best_acc:  85.16  a: 0.97\n",
            " [================================================================>]  Step: 59ms | Tot: 31s446ms | Loss: 0.429 | Acc: 85.826% (42913/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s399ms | Loss: 0.397 | Acc: 86.838% (43419/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s861ms | Loss: 0.456 | Acc: 85.120% (8512/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 36  Learning rate: [0.05]  best_acc:  85.16  a: 0.972\n",
            " [================================================================>]  Step: 65ms | Tot: 31s514ms | Loss: 0.416 | Acc: 86.130% (43065/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 16s945ms | Loss: 0.383 | Acc: 87.218% (43609/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s840ms | Loss: 0.451 | Acc: 85.150% (8515/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 37  Learning rate: [0.05]  best_acc:  85.16  a: 0.974\n",
            " [================================================================>]  Step: 61ms | Tot: 31s600ms | Loss: 0.401 | Acc: 86.618% (43309/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 16s312ms | Loss: 0.381 | Acc: 87.348% (43674/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s807ms | Loss: 0.447 | Acc: 85.680% (8568/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 38  Learning rate: [0.05]  best_acc:  85.68  a: 0.976\n",
            " [================================================================>]  Step: 57ms | Tot: 31s525ms | Loss: 0.393 | Acc: 86.930% (43465/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s263ms | Loss: 0.397 | Acc: 86.782% (43391/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s785ms | Loss: 0.464 | Acc: 84.540% (8454/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 39  Learning rate: [0.05]  best_acc:  85.68  a: 0.978\n",
            " [================================================================>]  Step: 61ms | Tot: 31s600ms | Loss: 0.391 | Acc: 87.078% (43539/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s345ms | Loss: 0.358 | Acc: 88.120% (44060/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s792ms | Loss: 0.424 | Acc: 86.050% (8605/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 40  Learning rate: [0.05]  best_acc:  86.05000000000001  a: 0.98\n",
            " [================================================================>]  Step: 63ms | Tot: 31s553ms | Loss: 0.376 | Acc: 87.534% (43767/50000) 391/391 \n",
            " [================================================================>]  Step: 30ms | Tot: 16s258ms | Loss: 0.357 | Acc: 88.148% (44074/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s928ms | Loss: 0.426 | Acc: 85.950% (8595/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 41  Learning rate: [0.05]  best_acc:  86.05000000000001  a: 0.982\n",
            " [================================================================>]  Step: 66ms | Tot: 31s679ms | Loss: 0.361 | Acc: 87.918% (43959/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s519ms | Loss: 0.343 | Acc: 88.592% (44296/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s806ms | Loss: 0.433 | Acc: 86.140% (8614/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 42  Learning rate: [0.05]  best_acc:  86.14  a: 0.984\n",
            " [================================================================>]  Step: 59ms | Tot: 31s561ms | Loss: 0.354 | Acc: 88.234% (44117/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s581ms | Loss: 0.337 | Acc: 88.842% (44421/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s759ms | Loss: 0.422 | Acc: 86.150% (8615/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 43  Learning rate: [0.05]  best_acc:  86.15  a: 0.986\n",
            " [================================================================>]  Step: 65ms | Tot: 31s385ms | Loss: 0.342 | Acc: 88.706% (44353/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s178ms | Loss: 0.326 | Acc: 88.994% (44497/50000) 391/391 \n",
            " [================================================================>]  Step: 30ms | Tot: 2s786ms | Loss: 0.409 | Acc: 87.140% (8714/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 44  Learning rate: [0.05]  best_acc:  87.14  a: 0.988\n",
            " [================================================================>]  Step: 63ms | Tot: 31s376ms | Loss: 0.327 | Acc: 89.052% (44526/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 16s500ms | Loss: 0.302 | Acc: 89.794% (44897/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s815ms | Loss: 0.394 | Acc: 87.400% (8740/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 45  Learning rate: [0.05]  best_acc:  87.4  a: 0.99\n",
            " [================================================================>]  Step: 59ms | Tot: 31s640ms | Loss: 0.317 | Acc: 89.488% (44744/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 16s460ms | Loss: 0.300 | Acc: 89.880% (44940/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s939ms | Loss: 0.412 | Acc: 86.710% (8671/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 46  Learning rate: [0.05]  best_acc:  87.4  a: 0.992\n",
            " [================================================================>]  Step: 62ms | Tot: 31s308ms | Loss: 0.296 | Acc: 90.146% (45073/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 16s51ms | Loss: 0.293 | Acc: 90.182% (45091/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 3s67ms | Loss: 0.413 | Acc: 86.530% (8653/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 47  Learning rate: [0.05]  best_acc:  87.4  a: 0.994\n",
            " [================================================================>]  Step: 61ms | Tot: 31s478ms | Loss: 0.293 | Acc: 90.264% (45132/50000) 391/391 \n",
            " [================================================================>]  Step: 30ms | Tot: 16s42ms | Loss: 0.268 | Acc: 91.040% (45520/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s895ms | Loss: 0.399 | Acc: 87.060% (8706/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 48  Learning rate: [0.05]  best_acc:  87.4  a: 0.996\n",
            " [================================================================>]  Step: 61ms | Tot: 31s342ms | Loss: 0.270 | Acc: 90.908% (45454/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 16s210ms | Loss: 0.251 | Acc: 91.570% (45785/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s814ms | Loss: 0.381 | Acc: 88.030% (8803/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 49  Learning rate: [0.05]  best_acc:  88.03  a: 0.998\n",
            " [================================================================>]  Step: 60ms | Tot: 31s591ms | Loss: 0.255 | Acc: 91.456% (45728/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 16s367ms | Loss: 0.242 | Acc: 91.882% (45941/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s742ms | Loss: 0.378 | Acc: 87.920% (8792/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 50  Learning rate: [0.05]  best_acc:  88.03  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s380ms | Loss: 0.239 | Acc: 92.068% (46034/50000) 391/391 \n",
            " [================================================================>]  Step: 18ms | Tot: 15s785ms | Loss: 0.229 | Acc: 92.258% (46129/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s673ms | Loss: 0.367 | Acc: 88.250% (8825/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 51  Learning rate: [0.05]  best_acc:  88.25  a: 1.0\n",
            " [================================================================>]  Step: 55ms | Tot: 31s391ms | Loss: 0.259 | Acc: 91.266% (45633/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 15s757ms | Loss: 0.258 | Acc: 91.332% (45666/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s777ms | Loss: 0.395 | Acc: 87.700% (8770/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 52  Learning rate: [0.07500000000000001]  best_acc:  88.25  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s418ms | Loss: 0.260 | Acc: 91.304% (45652/50000) 391/391 \n",
            " [================================================================>]  Step: 15ms | Tot: 15s930ms | Loss: 0.252 | Acc: 91.676% (45838/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s721ms | Loss: 0.393 | Acc: 87.940% (8794/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 53  Learning rate: [0.07500000000000001]  best_acc:  88.25  a: 1.0\n",
            " [================================================================>]  Step: 64ms | Tot: 31s673ms | Loss: 0.262 | Acc: 91.250% (45625/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s76ms | Loss: 0.247 | Acc: 91.584% (45792/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 2s793ms | Loss: 0.379 | Acc: 87.520% (8752/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 54  Learning rate: [0.07500000000000001]  best_acc:  88.25  a: 1.0\n",
            " [================================================================>]  Step: 65ms | Tot: 31s513ms | Loss: 0.255 | Acc: 91.480% (45740/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s81ms | Loss: 0.257 | Acc: 91.226% (45613/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s740ms | Loss: 0.390 | Acc: 87.460% (8746/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 55  Learning rate: [0.07500000000000001]  best_acc:  88.25  a: 1.0\n",
            " [================================================================>]  Step: 63ms | Tot: 31s227ms | Loss: 0.257 | Acc: 91.256% (45628/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 16s151ms | Loss: 0.240 | Acc: 92.014% (46007/50000) 391/391 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s815ms | Loss: 0.383 | Acc: 88.320% (8832/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 56  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s204ms | Loss: 0.255 | Acc: 91.488% (45744/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s712ms | Loss: 0.232 | Acc: 92.142% (46071/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 3s22ms | Loss: 0.378 | Acc: 88.060% (8806/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 57  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 56ms | Tot: 31s357ms | Loss: 0.259 | Acc: 91.260% (45630/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 16s268ms | Loss: 0.249 | Acc: 91.640% (45820/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 3s43ms | Loss: 0.405 | Acc: 87.540% (8754/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 58  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 56ms | Tot: 31s259ms | Loss: 0.254 | Acc: 91.512% (45756/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s593ms | Loss: 0.241 | Acc: 91.988% (45994/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s811ms | Loss: 0.394 | Acc: 87.780% (8778/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 59  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s367ms | Loss: 0.254 | Acc: 91.532% (45766/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s216ms | Loss: 0.235 | Acc: 92.076% (46038/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s762ms | Loss: 0.389 | Acc: 87.750% (8775/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 60  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 64ms | Tot: 31s540ms | Loss: 0.254 | Acc: 91.412% (45706/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 16s49ms | Loss: 0.247 | Acc: 91.814% (45907/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s646ms | Loss: 0.389 | Acc: 87.610% (8761/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 61  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 56ms | Tot: 31s594ms | Loss: 0.248 | Acc: 91.720% (45860/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 16s109ms | Loss: 0.235 | Acc: 92.142% (46071/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s753ms | Loss: 0.371 | Acc: 87.840% (8784/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 62  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 56ms | Tot: 31s537ms | Loss: 0.249 | Acc: 91.668% (45834/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s995ms | Loss: 0.233 | Acc: 92.144% (46072/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s759ms | Loss: 0.376 | Acc: 87.960% (8796/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 63  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s437ms | Loss: 0.248 | Acc: 91.616% (45808/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 15s917ms | Loss: 0.245 | Acc: 91.828% (45914/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s799ms | Loss: 0.402 | Acc: 87.430% (8743/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 64  Learning rate: [0.07500000000000001]  best_acc:  88.32  a: 1.0\n",
            " [================================================================>]  Step: 68ms | Tot: 31s343ms | Loss: 0.250 | Acc: 91.604% (45802/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 16s158ms | Loss: 0.234 | Acc: 92.020% (46010/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s765ms | Loss: 0.373 | Acc: 88.470% (8847/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 65  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s466ms | Loss: 0.254 | Acc: 91.538% (45769/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 16s357ms | Loss: 0.232 | Acc: 92.220% (46110/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s770ms | Loss: 0.386 | Acc: 88.060% (8806/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 66  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s577ms | Loss: 0.246 | Acc: 91.752% (45876/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 16s497ms | Loss: 0.225 | Acc: 92.386% (46193/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s888ms | Loss: 0.380 | Acc: 88.030% (8803/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 67  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 58ms | Tot: 31s432ms | Loss: 0.245 | Acc: 91.814% (45907/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s513ms | Loss: 0.233 | Acc: 92.202% (46101/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s951ms | Loss: 0.383 | Acc: 88.220% (8822/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 68  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 63ms | Tot: 31s480ms | Loss: 0.253 | Acc: 91.548% (45774/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s142ms | Loss: 0.225 | Acc: 92.484% (46242/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 3s141ms | Loss: 0.385 | Acc: 87.850% (8785/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 69  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 55ms | Tot: 31s400ms | Loss: 0.245 | Acc: 91.924% (45962/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s38ms | Loss: 0.238 | Acc: 92.076% (46038/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s926ms | Loss: 0.386 | Acc: 87.520% (8752/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 70  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 64ms | Tot: 31s535ms | Loss: 0.251 | Acc: 91.512% (45756/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 16s147ms | Loss: 0.241 | Acc: 91.938% (45969/50000) 391/391 \n",
            " [================================================================>]  Step: 30ms | Tot: 2s903ms | Loss: 0.394 | Acc: 87.480% (8748/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 71  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s488ms | Loss: 0.241 | Acc: 91.940% (45970/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 16s359ms | Loss: 0.227 | Acc: 92.354% (46177/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s709ms | Loss: 0.381 | Acc: 87.940% (8794/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 72  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s520ms | Loss: 0.251 | Acc: 91.642% (45821/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 16s10ms | Loss: 0.240 | Acc: 92.056% (46028/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s693ms | Loss: 0.388 | Acc: 87.820% (8782/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 73  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s698ms | Loss: 0.251 | Acc: 91.584% (45792/50000) 391/391 \n",
            " [================================================================>]  Step: 16ms | Tot: 16s151ms | Loss: 0.226 | Acc: 92.460% (46230/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s685ms | Loss: 0.393 | Acc: 87.660% (8766/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 74  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s563ms | Loss: 0.247 | Acc: 91.608% (45804/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 16s611ms | Loss: 0.249 | Acc: 91.708% (45854/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s667ms | Loss: 0.409 | Acc: 87.310% (8731/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 75  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 64ms | Tot: 31s446ms | Loss: 0.242 | Acc: 91.994% (45997/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s170ms | Loss: 0.234 | Acc: 92.340% (46170/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s732ms | Loss: 0.388 | Acc: 87.370% (8737/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 76  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 72ms | Tot: 31s529ms | Loss: 0.248 | Acc: 91.680% (45840/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s37ms | Loss: 0.245 | Acc: 91.788% (45894/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s717ms | Loss: 0.413 | Acc: 87.080% (8708/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 77  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s331ms | Loss: 0.238 | Acc: 92.072% (46036/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 16s347ms | Loss: 0.219 | Acc: 92.586% (46293/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s785ms | Loss: 0.390 | Acc: 87.850% (8785/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 78  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 63ms | Tot: 31s386ms | Loss: 0.245 | Acc: 91.782% (45891/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s75ms | Loss: 0.230 | Acc: 92.312% (46156/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s843ms | Loss: 0.389 | Acc: 88.040% (8804/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 79  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s561ms | Loss: 0.243 | Acc: 91.798% (45899/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s213ms | Loss: 0.236 | Acc: 92.232% (46116/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s926ms | Loss: 0.393 | Acc: 87.780% (8778/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 80  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s453ms | Loss: 0.236 | Acc: 92.120% (46060/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s175ms | Loss: 0.226 | Acc: 92.418% (46209/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 3s1ms | Loss: 0.398 | Acc: 87.490% (8749/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 81  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s489ms | Loss: 0.243 | Acc: 91.836% (45918/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 16s123ms | Loss: 0.215 | Acc: 92.862% (46431/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s937ms | Loss: 0.386 | Acc: 87.810% (8781/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 82  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s512ms | Loss: 0.245 | Acc: 91.838% (45919/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 16s294ms | Loss: 0.240 | Acc: 91.920% (45960/50000) 391/391 \n",
            " [================================================================>]  Step: 31ms | Tot: 2s878ms | Loss: 0.422 | Acc: 86.860% (8686/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 83  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s653ms | Loss: 0.239 | Acc: 91.888% (45944/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s302ms | Loss: 0.225 | Acc: 92.394% (46197/50000) 391/391 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s762ms | Loss: 0.383 | Acc: 88.000% (8800/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 84  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 69ms | Tot: 31s640ms | Loss: 0.235 | Acc: 92.064% (46032/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s132ms | Loss: 0.230 | Acc: 92.292% (46146/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s717ms | Loss: 0.404 | Acc: 87.340% (8734/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 85  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s574ms | Loss: 0.233 | Acc: 92.200% (46100/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 16s186ms | Loss: 0.235 | Acc: 92.236% (46118/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s730ms | Loss: 0.415 | Acc: 87.570% (8757/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 86  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 58ms | Tot: 31s568ms | Loss: 0.238 | Acc: 91.998% (45999/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 16s51ms | Loss: 0.219 | Acc: 92.668% (46334/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 2s783ms | Loss: 0.394 | Acc: 87.960% (8796/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 87  Learning rate: [0.07500000000000001]  best_acc:  88.47  a: 1.0\n",
            " [================================================================>]  Step: 67ms | Tot: 31s951ms | Loss: 0.237 | Acc: 92.012% (46006/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s142ms | Loss: 0.213 | Acc: 92.948% (46474/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 2s799ms | Loss: 0.371 | Acc: 88.800% (8880/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 88  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 73ms | Tot: 31s552ms | Loss: 0.238 | Acc: 92.024% (46012/50000) 391/391 \n",
            " [================================================================>]  Step: 18ms | Tot: 16s542ms | Loss: 0.221 | Acc: 92.606% (46303/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s714ms | Loss: 0.395 | Acc: 87.810% (8781/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 89  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s497ms | Loss: 0.238 | Acc: 91.894% (45947/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 16s315ms | Loss: 0.224 | Acc: 92.516% (46258/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s737ms | Loss: 0.403 | Acc: 87.730% (8773/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 90  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 57ms | Tot: 31s465ms | Loss: 0.233 | Acc: 92.102% (46051/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s238ms | Loss: 0.222 | Acc: 92.464% (46232/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s762ms | Loss: 0.396 | Acc: 87.610% (8761/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 91  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 56ms | Tot: 31s434ms | Loss: 0.235 | Acc: 92.040% (46020/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 17s105ms | Loss: 0.206 | Acc: 93.136% (46568/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s776ms | Loss: 0.389 | Acc: 88.230% (8823/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 92  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 58ms | Tot: 31s355ms | Loss: 0.234 | Acc: 92.064% (46032/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 16s55ms | Loss: 0.229 | Acc: 92.324% (46162/50000) 391/391 \n",
            " [================================================================>]  Step: 18ms | Tot: 2s859ms | Loss: 0.389 | Acc: 88.090% (8809/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 93  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s460ms | Loss: 0.235 | Acc: 92.130% (46065/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 15s784ms | Loss: 0.214 | Acc: 92.792% (46396/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 3s98ms | Loss: 0.388 | Acc: 88.090% (8809/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 94  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 54ms | Tot: 31s410ms | Loss: 0.233 | Acc: 92.266% (46133/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s310ms | Loss: 0.216 | Acc: 92.644% (46322/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s981ms | Loss: 0.398 | Acc: 87.890% (8789/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 95  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s456ms | Loss: 0.230 | Acc: 92.284% (46142/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 16s190ms | Loss: 0.221 | Acc: 92.666% (46333/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 2s776ms | Loss: 0.389 | Acc: 87.580% (8758/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 96  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s645ms | Loss: 0.226 | Acc: 92.418% (46209/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s281ms | Loss: 0.219 | Acc: 92.654% (46327/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s800ms | Loss: 0.383 | Acc: 87.590% (8759/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 97  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s624ms | Loss: 0.222 | Acc: 92.550% (46275/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 15s912ms | Loss: 0.214 | Acc: 92.860% (46430/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s699ms | Loss: 0.400 | Acc: 87.880% (8788/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 98  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s501ms | Loss: 0.233 | Acc: 92.226% (46113/50000) 391/391 \n",
            " [================================================================>]  Step: 18ms | Tot: 15s865ms | Loss: 0.210 | Acc: 92.952% (46476/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s667ms | Loss: 0.388 | Acc: 88.190% (8819/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 99  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s619ms | Loss: 0.230 | Acc: 92.240% (46120/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s154ms | Loss: 0.233 | Acc: 92.184% (46092/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s814ms | Loss: 0.412 | Acc: 87.280% (8728/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 100  Learning rate: [0.07500000000000001]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 70ms | Tot: 31s724ms | Loss: 0.227 | Acc: 92.420% (46210/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s1ms | Loss: 0.216 | Acc: 92.760% (46380/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s754ms | Loss: 0.399 | Acc: 87.390% (8739/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 101  Learning rate: [0.0075000000000000015]  best_acc:  88.8  a: 1.0\n",
            " [================================================================>]  Step: 66ms | Tot: 31s415ms | Loss: 0.168 | Acc: 94.330% (47165/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 16s465ms | Loss: 0.144 | Acc: 95.252% (47626/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 2s839ms | Loss: 0.331 | Acc: 89.880% (8988/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 102  Learning rate: [0.0075000000000000015]  best_acc:  89.88000000000001  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s487ms | Loss: 0.147 | Acc: 95.216% (47608/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s256ms | Loss: 0.139 | Acc: 95.484% (47742/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 2s858ms | Loss: 0.328 | Acc: 89.700% (8970/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 103  Learning rate: [0.0075000000000000015]  best_acc:  89.88000000000001  a: 1.0\n",
            " [================================================================>]  Step: 64ms | Tot: 31s443ms | Loss: 0.134 | Acc: 95.454% (47727/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 16s323ms | Loss: 0.130 | Acc: 95.696% (47848/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s854ms | Loss: 0.326 | Acc: 90.080% (9008/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 104  Learning rate: [0.0075000000000000015]  best_acc:  90.08  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s559ms | Loss: 0.130 | Acc: 95.710% (47855/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 16s712ms | Loss: 0.125 | Acc: 95.964% (47982/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 2s861ms | Loss: 0.340 | Acc: 89.930% (8993/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 105  Learning rate: [0.0075000000000000015]  best_acc:  90.08  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s431ms | Loss: 0.128 | Acc: 95.662% (47831/50000) 391/391 \n",
            " [================================================================>]  Step: 27ms | Tot: 16s159ms | Loss: 0.127 | Acc: 95.792% (47896/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 3s24ms | Loss: 0.339 | Acc: 89.790% (8979/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 106  Learning rate: [0.0075000000000000015]  best_acc:  90.08  a: 1.0\n",
            " [================================================================>]  Step: 59ms | Tot: 31s426ms | Loss: 0.123 | Acc: 95.862% (47931/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 16s98ms | Loss: 0.121 | Acc: 95.990% (47995/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 3s85ms | Loss: 0.342 | Acc: 89.850% (8985/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 107  Learning rate: [0.0075000000000000015]  best_acc:  90.08  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s457ms | Loss: 0.123 | Acc: 95.898% (47949/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 16s331ms | Loss: 0.120 | Acc: 96.006% (48003/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 3s115ms | Loss: 0.338 | Acc: 89.960% (8996/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 108  Learning rate: [0.0075000000000000015]  best_acc:  90.08  a: 1.0\n",
            " [================================================================>]  Step: 63ms | Tot: 31s594ms | Loss: 0.118 | Acc: 96.034% (48017/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 16s108ms | Loss: 0.114 | Acc: 96.222% (48111/50000) 391/391 \n",
            " [================================================================>]  Step: 34ms | Tot: 2s982ms | Loss: 0.345 | Acc: 89.830% (8983/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 109  Learning rate: [0.0075000000000000015]  best_acc:  90.08  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s381ms | Loss: 0.120 | Acc: 96.024% (48012/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 15s815ms | Loss: 0.113 | Acc: 96.316% (48158/50000) 391/391 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s846ms | Loss: 0.343 | Acc: 90.100% (9010/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 110  Learning rate: [0.0075000000000000015]  best_acc:  90.10000000000001  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s290ms | Loss: 0.118 | Acc: 96.058% (48029/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s758ms | Loss: 0.114 | Acc: 96.196% (48098/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 2s802ms | Loss: 0.338 | Acc: 89.910% (8991/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 111  Learning rate: [0.0075000000000000015]  best_acc:  90.10000000000001  a: 1.0\n",
            " [================================================================>]  Step: 66ms | Tot: 31s346ms | Loss: 0.115 | Acc: 96.164% (48082/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 15s730ms | Loss: 0.111 | Acc: 96.356% (48178/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s751ms | Loss: 0.343 | Acc: 89.910% (8991/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 112  Learning rate: [0.0075000000000000015]  best_acc:  90.10000000000001  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s538ms | Loss: 0.112 | Acc: 96.272% (48136/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 15s690ms | Loss: 0.106 | Acc: 96.476% (48238/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s746ms | Loss: 0.342 | Acc: 90.220% (9022/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 113  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 66ms | Tot: 31s348ms | Loss: 0.113 | Acc: 96.212% (48106/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 15s787ms | Loss: 0.110 | Acc: 96.380% (48190/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s855ms | Loss: 0.346 | Acc: 90.200% (9020/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 114  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 67ms | Tot: 31s344ms | Loss: 0.108 | Acc: 96.436% (48218/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s67ms | Loss: 0.107 | Acc: 96.466% (48233/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s783ms | Loss: 0.354 | Acc: 89.930% (8993/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 115  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s276ms | Loss: 0.109 | Acc: 96.286% (48143/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 16s251ms | Loss: 0.105 | Acc: 96.494% (48247/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s828ms | Loss: 0.348 | Acc: 89.780% (8978/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 116  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 55ms | Tot: 31s452ms | Loss: 0.111 | Acc: 96.278% (48139/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 16s402ms | Loss: 0.102 | Acc: 96.648% (48324/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s997ms | Loss: 0.351 | Acc: 89.940% (8994/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 117  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s354ms | Loss: 0.105 | Acc: 96.458% (48229/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 15s758ms | Loss: 0.102 | Acc: 96.618% (48309/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 3s81ms | Loss: 0.350 | Acc: 89.920% (8992/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 118  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s371ms | Loss: 0.104 | Acc: 96.532% (48266/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 15s639ms | Loss: 0.102 | Acc: 96.702% (48351/50000) 391/391 \n",
            " [================================================================>]  Step: 25ms | Tot: 2s991ms | Loss: 0.352 | Acc: 89.920% (8992/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 119  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s287ms | Loss: 0.106 | Acc: 96.494% (48247/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 15s649ms | Loss: 0.102 | Acc: 96.656% (48328/50000) 391/391 \n",
            " [================================================================>]  Step: 34ms | Tot: 2s856ms | Loss: 0.356 | Acc: 89.700% (8970/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 120  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 60ms | Tot: 31s624ms | Loss: 0.105 | Acc: 96.494% (48247/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s841ms | Loss: 0.099 | Acc: 96.768% (48384/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 2s779ms | Loss: 0.358 | Acc: 89.980% (8998/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 121  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 68ms | Tot: 31s467ms | Loss: 0.101 | Acc: 96.532% (48266/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 15s931ms | Loss: 0.099 | Acc: 96.642% (48321/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s830ms | Loss: 0.358 | Acc: 89.820% (8982/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 122  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s443ms | Loss: 0.100 | Acc: 96.662% (48331/50000) 391/391 \n",
            " [================================================================>]  Step: 21ms | Tot: 15s934ms | Loss: 0.099 | Acc: 96.764% (48382/50000) 391/391 \n",
            " [================================================================>]  Step: 24ms | Tot: 2s757ms | Loss: 0.366 | Acc: 89.560% (8956/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 123  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 61ms | Tot: 31s232ms | Loss: 0.101 | Acc: 96.712% (48356/50000) 391/391 \n",
            " [================================================================>]  Step: 17ms | Tot: 15s678ms | Loss: 0.096 | Acc: 96.820% (48410/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s690ms | Loss: 0.362 | Acc: 89.870% (8987/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 124  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 63ms | Tot: 31s407ms | Loss: 0.100 | Acc: 96.610% (48305/50000) 391/391 \n",
            " [================================================================>]  Step: 18ms | Tot: 15s429ms | Loss: 0.098 | Acc: 96.618% (48309/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 2s757ms | Loss: 0.360 | Acc: 89.940% (8994/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 125  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 64ms | Tot: 31s381ms | Loss: 0.099 | Acc: 96.708% (48354/50000) 391/391 \n",
            " [================================================================>]  Step: 19ms | Tot: 16s24ms | Loss: 0.094 | Acc: 96.800% (48400/50000) 391/391 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s734ms | Loss: 0.361 | Acc: 89.860% (8986/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 126  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 62ms | Tot: 31s235ms | Loss: 0.097 | Acc: 96.710% (48355/50000) 391/391 \n",
            " [================================================================>]  Step: 28ms | Tot: 15s985ms | Loss: 0.094 | Acc: 96.780% (48390/50000) 391/391 \n",
            " [================================================================>]  Step: 23ms | Tot: 2s783ms | Loss: 0.363 | Acc: 89.800% (8980/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 127  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n",
            " [================================================================>]  Step: 63ms | Tot: 31s350ms | Loss: 0.096 | Acc: 96.770% (48385/50000) 391/391 \n",
            " [================================================================>]  Step: 26ms | Tot: 16s245ms | Loss: 0.094 | Acc: 96.904% (48452/50000) 391/391 \n",
            " [================================================================>]  Step: 22ms | Tot: 3s28ms | Loss: 0.376 | Acc: 89.620% (8962/10000) 100/100 \n",
            "Saving..\n",
            "\n",
            "Epoch: 128  Learning rate: [0.0075000000000000015]  best_acc:  90.22  a: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN2KLZy_u122"
      },
      "outputs": [],
      "source": [
        "for target_epoch in [200]:\n",
        "  for a in [0.9,0.7,0.5,0.3,0.1]:\n",
        "    for lr in [0.0007,0.001,0.002,0.003,0.004,0.005]:\n",
        "      LinearCombineSGDMAdam_dynamic_weighting(target_epoch=target_epoch,lr=lr,lr_sgdm='NA', dataset='CIFAR10',a=a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzMVddjrVnb5"
      },
      "outputs": [],
      "source": [
        "# [0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05,0.06,0.07,0.001,0.003,0.005,0.007,0.009,0.01,0.09,0.1,0.11]\n",
        "for switch in [50]:\n",
        "  for lr in [0.005,0.003,0.001]:\n",
        "    for lr_sgdm in [0.009,0.007,0.005,0.003,0.001,0.01,0.03,0.05,0.07,0.09,0.1,0.11]:\n",
        "      LinearCombineSGDMAdam_switch(switch=switch,lr=lr,lr_sgdm=lr_sgdm,dataset='CIFAR10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpWwu3z1WbJz"
      },
      "outputs": [],
      "source": [
        "LinearCombineSGDMAdam_switch(switch=25,lr=0.003,lr_sgdm=0.03,dataset='CIFAR100')\n",
        "\n",
        "# LinearCombineSGDMAdam_a=0.7_lr=0.003_switch=25_lr_sgdm=0.03 best acc: 94.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9JORXlw7L72"
      },
      "outputs": [],
      "source": [
        "for switch in [25]: # done\n",
        "  for lr in [0.001,0.003,0.005]:\n",
        "    for lr_sgdm in [0.01,0.03,0.05,0.07,0.09,0.1]:  #0.001,0.003,0.005,0.007,0.009,0.11\n",
        "      LinearCombineSGDMAdam_switch(switch=switch,lr=lr,lr_sgdm=lr_sgdm,dataset='CIFAR10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9yWacHZUTyX"
      },
      "outputs": [],
      "source": [
        "LinearCombineSGDMAdam_switch(switch=0,lr=0.001,lr_sgdm=0.05,dataset='CIFAR100')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM3_OdXKnBws"
      },
      "outputs": [],
      "source": [
        "# no switch\n",
        "for lr in [0.003,0.005,0.007,0.009,0.1,0.01,0.03,0.05,0.07,0.09,0.001,0.11]:\n",
        "    LinearCombineSGDMAdam_switch(switch=200,lr=lr,lr_sgdm=0,dataset='CIFAR10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOEDL0AXMlT1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppvx9g1uv8-x"
      },
      "source": [
        "# Completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v2H1IQSJnBU"
      },
      "outputs": [],
      "source": [
        "LinearCombineSGDMAdam_switch(switch=50,lr_sgdm=0.05,dataset='CIFAR100')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4ff77VtJyzc"
      },
      "outputs": [],
      "source": [
        "LinearCombineSGDMAdam_switch(switch=100,lr_sgdm=0.03,dataset='CIFAR100')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmFBwOxrMEWl"
      },
      "outputs": [],
      "source": [
        "for switch in [100]:\n",
        "  for lr_sgdm in [0.001,0.003,0.005,0.007,0.009,0.01,0.03,0.05,0.07,0.09,0.1,0.11]:\n",
        "    LinearCombineSGDMAdam_switch(switch,lr_sgdm,'CIFAR10')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBKTiu3-KSZs"
      },
      "source": [
        "# Backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bxllsKRrmNWz"
      },
      "outputs": [],
      "source": [
        "#@title CIFAR10 Training with linear combination of Padam and sgdm\n",
        "\n",
        "# Hyperparameters ##############################################\n",
        "\n",
        "# Tuning Hyperparameters\n",
        "lr=0.001 # lr for LinearCombineAdamSGDM\n",
        "a=0.7 # LinearCombinePadamSGDM weight for SGDM.  weight for PAdam is 1-a\n",
        "partial=1/8\n",
        "\n",
        "# Fixed Hyperparameters\n",
        "net=\"vggnet\"\n",
        "method=f\"LinearCombineSGDMPadam_a={a}_lr={lr}_partial={partial}\"\n",
        "wd = 5e-4\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "Nepoch=200\n",
        "resume=True\n",
        "\n",
        "\n",
        "print(method)\n",
        "\n",
        "# init #########################################################\n",
        "results_path=f'{path}/checkpoint/{net}/CIFAR10/{method}'\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "train_errs = []\n",
        "test_errs = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "lrs = []\n",
        "\n",
        "# Data #########################################################\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=f'{path}/data/', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=f'{path}/data/', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "            'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# Model #########################################################\n",
        "if resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    try:\n",
        "        checkpoint = torch.load(results_path)\n",
        "        model = checkpoint['model']\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        train_errs = checkpoint['train_errs']\n",
        "        test_losses = checkpoint['test_losses']\n",
        "        test_errs = checkpoint['test_errs']\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        lrs = checkpoint['lrs']\n",
        "        lr=lrs[-1]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print('==> Building model..')\n",
        "        if net == 'vggnet':\n",
        "            model = VGG('VGG16', num_classes=10)\n",
        "        # #     model = models.vgg16_bn(num_classes=10)\n",
        "        # elif net == 'resnet':\n",
        "        #     model = ResNet18(num_classes=10)\n",
        "        # #     model = models.resnet18(num_classes=10)\n",
        "        # elif net == 'wideresnet':\n",
        "        #     model = WResNet_cifar10(\n",
        "        #         num_classes=10, depth=16, multiplier=4)\n",
        "        else:\n",
        "            print('Network undefined!')\n",
        "else:\n",
        "    print('==> Building model..')\n",
        "    if net == 'vggnet':\n",
        "        model = VGG('VGG16', num_classes=10)\n",
        "    # #     model = models.vgg16_bn(num_classes=10)\n",
        "    # elif net == 'resnet':\n",
        "    #     model = ResNet18(num_classes=10)\n",
        "    # #     model = models.resnet18(num_classes=10)\n",
        "    # elif net == 'wideresnet':\n",
        "    #     model = WResNet_cifar10(\n",
        "    #         num_classes=10, depth=16, multiplier=4)\n",
        "    else:\n",
        "        print('Network undefined!')\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "    model = torch.nn.DataParallel(\n",
        "        model, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Optimize/Train #####################################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "betas = (beta1, beta2)\n",
        "optimizer = LinearCombineSGDMAdam(model.parameters(), lr=lr, weight_decay = wd, betas = betas, a=a,partial=partial)\n",
        "milestones=[100-start_epoch,150-start_epoch]\n",
        "print(milestones)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[i for i in milestones if i>0], gamma=0.1)\n",
        "\n",
        "for epoch in range(start_epoch+1, Nepoch+1):\n",
        "    print('\\nEpoch: %d' % epoch, ' Learning rate:', scheduler.get_last_lr(), ' best_acc: ', best_acc)\n",
        "    # Train\n",
        "    train_loop(trainloader=trainloader, model=model, \n",
        "                criterion=criterion, optimizer=optimizer,\n",
        "                use_cuda=use_cuda,train_errs=train_errs,train_losses=train_losses)\n",
        "    # Test\n",
        "    test_loop(testloader=testloader, model=model, criterion=criterion, \n",
        "              use_cuda=use_cuda, test_errs=test_errs, test_losses=test_losses)\n",
        "    # Save checkpoint\n",
        "    acc = 100.0 * (1-test_errs[-1])\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "    lrs.extend(scheduler.get_last_lr())\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'model': model,\n",
        "        'epoch': epoch,\n",
        "        'best_acc': best_acc,\n",
        "        'train_errs': train_errs,\n",
        "        'train_losses': train_losses,\n",
        "        'test_errs': test_errs,\n",
        "        'test_losses': test_losses,\n",
        "        'lrs': lrs\n",
        "    }\n",
        "    torch.save(state, results_path)\n",
        "\n",
        "    scheduler.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6EfwjSHBVT2P"
      },
      "outputs": [],
      "source": [
        "#@title CIFAR10 Training with switch between SGDM and ADAM\n",
        "\n",
        "# Hyperparameters ##############################################\n",
        "\n",
        "# Tuning Hyperparameters\n",
        "switch=50 # switch to SGDM\n",
        "lr=0.001 # lr for LinearCombineAdamSGDM\n",
        "lr_sgdm=0.05\n",
        "a=0.7 # LinearCombineAdamSGDM weight for SGDM.  weight for Adam is 1-a\n",
        "\n",
        "# Fixed Hyperparameters\n",
        "net=\"vggnet\"\n",
        "method=f\"LinearCombineSGDMAdam_a={a}_lr={lr}_switch={switch}_lr_sgdm={lr_sgdm}\"\n",
        "wd = 5e-4\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "Nepoch=200\n",
        "resume=True\n",
        "\n",
        "print(method)\n",
        "\n",
        "# init #########################################################\n",
        "results_path=f'{path}/checkpoint/{net}/CIFAR10/{method}'\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "train_errs = []\n",
        "test_errs = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "lrs = []\n",
        "\n",
        "# Data #########################################################\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=f'{path}/data/', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=f'{path}/data/', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "            'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# Model #########################################################\n",
        "if resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    try:\n",
        "        checkpoint = torch.load(results_path)\n",
        "        model = checkpoint['model']\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        train_errs = checkpoint['train_errs']\n",
        "        test_losses = checkpoint['test_losses']\n",
        "        test_errs = checkpoint['test_errs']\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        lrs = checkpoint['lrs']\n",
        "        lr=lrs[-1]\n",
        "        if start_epoch > switch:\n",
        "          lr_sgdm = lr\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print('==> Building model..')\n",
        "        if net == 'vggnet':\n",
        "            model = VGG('VGG16', num_classes=10)\n",
        "        # #     model = models.vgg16_bn(num_classes=10)\n",
        "        # elif net == 'resnet':\n",
        "        #     model = ResNet18(num_classes=10)\n",
        "        # #     model = models.resnet18(num_classes=10)\n",
        "        # elif net == 'wideresnet':\n",
        "        #     model = WResNet_cifar10(\n",
        "        #         num_classes=10, depth=16, multiplier=4)\n",
        "        else:\n",
        "            print('Network undefined!')\n",
        "else:\n",
        "    print('==> Building model..')\n",
        "    if net == 'vggnet':\n",
        "        model = VGG('VGG16', num_classes=10)\n",
        "    # #     model = models.vgg16_bn(num_classes=10)\n",
        "    # elif net == 'resnet':\n",
        "    #     model = ResNet18(num_classes=10)\n",
        "    # #     model = models.resnet18(num_classes=10)\n",
        "    # elif net == 'wideresnet':\n",
        "    #     model = WResNet_cifar10(\n",
        "    #         num_classes=10, depth=16, multiplier=4)\n",
        "    else:\n",
        "        print('Network undefined!')\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "    model = torch.nn.DataParallel(\n",
        "        model, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Optimize/Train #####################################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "betas = (beta1, beta2)\n",
        "optimizer = LinearCombineSGDMAdam(model.parameters(), lr=lr, weight_decay = wd, betas = betas, a=a)\n",
        "milestones=[100-start_epoch,150-start_epoch]\n",
        "# milestones=[150-start_epoch]\n",
        "scheduler = MultiStepLR(optimizer, milestones=[i for i in milestones if i>0], gamma=0.1)\n",
        "\n",
        "for epoch in range(start_epoch+1, switch+1):\n",
        "    print('\\nEpoch: %d' % epoch, ' Learning rate:', scheduler.get_last_lr(), ' best_acc: ', best_acc)\n",
        "    # Train\n",
        "    train_loop(trainloader=trainloader, model=model, \n",
        "                criterion=criterion, optimizer=optimizer,\n",
        "                use_cuda=use_cuda,train_errs=train_errs,train_losses=train_losses)\n",
        "    # Test\n",
        "    test_loop(testloader=testloader, model=model, criterion=criterion, \n",
        "              use_cuda=use_cuda, test_errs=test_errs, test_losses=test_losses)\n",
        "    # Save checkpoint\n",
        "    acc = 100.0 * (1-test_errs[-1])\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "    lrs.extend(scheduler.get_last_lr())\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'model': model,\n",
        "        'epoch': epoch,\n",
        "        'best_acc': best_acc,\n",
        "        'train_errs': train_errs,\n",
        "        'train_losses': train_losses,\n",
        "        'test_errs': test_errs,\n",
        "        'test_losses': test_losses,\n",
        "        'lrs': lrs\n",
        "    }\n",
        "    torch.save(state, results_path)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "if lr_sgdm > 0:\n",
        "    # Switch to SGDM\n",
        "    print('============ Switch to sgdm ==========')\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr_sgdm, momentum= 0.9, weight_decay = wd)\n",
        "    milestones=[100-max(switch,start_epoch),150-max(switch,start_epoch)]\n",
        "    # milestones=[150-max(switch,start_epoch)]\n",
        "    print([i for i in milestones if i>0])\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[i for i in milestones if i>0], gamma=0.1)\n",
        "\n",
        "    for epoch in range(max(switch+1,start_epoch+1), 200+1):\n",
        "        print('\\nEpoch: %d' % epoch, ' Learning rate:', scheduler.get_last_lr(), ' best_acc: ', best_acc)\n",
        "        # Train\n",
        "        train_loop(trainloader=trainloader, model=model, \n",
        "                    criterion=criterion, optimizer=optimizer,\n",
        "                    use_cuda=use_cuda,train_errs=train_errs,train_losses=train_losses)\n",
        "        # Test\n",
        "        test_loop(testloader=testloader, model=model, criterion=criterion, \n",
        "                  use_cuda=use_cuda, test_errs=test_errs, test_losses=test_losses)\n",
        "        # Save checkpoint\n",
        "        acc = 100.0 * (1-test_errs[-1])\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "        lrs.extend(scheduler.get_last_lr())\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'model': model,\n",
        "            'epoch': epoch,\n",
        "            'best_acc': best_acc,\n",
        "            'train_errs': train_errs,\n",
        "            'train_losses': train_losses,\n",
        "            'test_errs': test_errs,\n",
        "            'test_losses': test_losses,\n",
        "            'lrs': lrs\n",
        "        }\n",
        "        torch.save(state, results_path)\n",
        "\n",
        "        scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bpV96iA-t0G_"
      },
      "outputs": [],
      "source": [
        "#@title CIFAR10 Training with dynamic weighting of SGDM and ADAM\n",
        "\n",
        "# Hyperparameters ##############################################\n",
        "\n",
        "# Tuning Hyperparameters\n",
        "target_epoch=50 # target_epoch for a to meet 1 (i.e. switch to SGDM).\n",
        "lr=0.05 # lr for LinearCombineAdamSGDM\n",
        "a=0.7 # LinearCombineAdamSGDM weight for SGDM.  weight for Adam is 1-a\n",
        "\n",
        "\n",
        "# Fixed Hyperparameters\n",
        "net=\"vggnet\"\n",
        "method=f\"LinearCombineSGDMAdam_a={a}_lr={lr}_target_epoch={target_epoch}\"\n",
        "wd = 5e-4\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "Nepoch=200\n",
        "resume=True\n",
        "\n",
        "\n",
        "print(method)\n",
        "\n",
        "# init #########################################################\n",
        "results_path=f'{path}/checkpoint/{net}/CIFAR10/{method}'\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "train_errs = []\n",
        "test_errs = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "lrs = []\n",
        "\n",
        "# Data #########################################################\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=f'{path}/data/', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=f'{path}/data/', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "            'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "# Model #########################################################\n",
        "if resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    try:\n",
        "        checkpoint = torch.load(results_path)\n",
        "        model = checkpoint['model']\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        train_errs = checkpoint['train_errs']\n",
        "        test_losses = checkpoint['test_losses']\n",
        "        test_errs = checkpoint['test_errs']\n",
        "        best_acc = checkpoint['best_acc']\n",
        "        lrs = checkpoint['lrs']\n",
        "        lr=lrs[-1]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print('==> Building model..')\n",
        "        if net == 'vggnet':\n",
        "            model = VGG('VGG16', num_classes=10)\n",
        "        # #     model = models.vgg16_bn(num_classes=10)\n",
        "        # elif net == 'resnet':\n",
        "        #     model = ResNet18(num_classes=10)\n",
        "        # #     model = models.resnet18(num_classes=10)\n",
        "        # elif net == 'wideresnet':\n",
        "        #     model = WResNet_cifar10(\n",
        "        #         num_classes=10, depth=16, multiplier=4)\n",
        "        else:\n",
        "            print('Network undefined!')\n",
        "else:\n",
        "    print('==> Building model..')\n",
        "    if net == 'vggnet':\n",
        "        model = VGG('VGG16', num_classes=10)\n",
        "    # #     model = models.vgg16_bn(num_classes=10)\n",
        "    # elif net == 'resnet':\n",
        "    #     model = ResNet18(num_classes=10)\n",
        "    # #     model = models.resnet18(num_classes=10)\n",
        "    # elif net == 'wideresnet':\n",
        "    #     model = WResNet_cifar10(\n",
        "    #         num_classes=10, depth=16, multiplier=4)\n",
        "    else:\n",
        "        print('Network undefined!')\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "    model = torch.nn.DataParallel(\n",
        "        model, device_ids=range(torch.cuda.device_count()))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Optimize/Train #####################################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "betas = (beta1, beta2)\n",
        "optimizer = LinearCombineSGDMAdam(model.parameters(), lr=lr, weight_decay = wd, betas = betas, a=a)\n",
        "milestones=[100-start_epoch,150-start_epoch]\n",
        "print(milestones)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[i for i in milestones if i>0], gamma=0.1)\n",
        "\n",
        "for epoch in range(start_epoch+1, Nepoch+1):\n",
        "    print('\\nEpoch: %d' % epoch, ' Learning rate:', scheduler.get_last_lr(), ' best_acc: ', best_acc, ' a:',a + min(epoch/target_epoch,1) * (1-a))\n",
        "    # Train\n",
        "    train_loop(trainloader=trainloader, model=model, \n",
        "                criterion=criterion, optimizer=optimizer,\n",
        "                use_cuda=use_cuda,train_errs=train_errs,train_losses=train_losses,epoch=epoch,target_epoch=target_epoch)\n",
        "    # Test\n",
        "    test_loop(testloader=testloader, model=model, criterion=criterion, \n",
        "              use_cuda=use_cuda, test_errs=test_errs, test_losses=test_losses)\n",
        "    # Save checkpoint\n",
        "    acc = 100.0 * (1-test_errs[-1])\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "    lrs.extend(scheduler.get_last_lr())\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'model': model,\n",
        "        'epoch': epoch,\n",
        "        'best_acc': best_acc,\n",
        "        'train_errs': train_errs,\n",
        "        'train_losses': train_losses,\n",
        "        'test_errs': test_errs,\n",
        "        'test_losses': test_losses,\n",
        "        'lrs': lrs\n",
        "    }\n",
        "    torch.save(state, results_path)\n",
        "\n",
        "    scheduler.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wkTe_eS0GvUJ"
      },
      "outputs": [],
      "source": [
        "#@title plot\n",
        "def plot_re(methods, dataset, net='vggnet'):\n",
        "    checkpoints={}\n",
        "    for i in methods:\n",
        "      checkpoints[i]=torch.load(f'{path}checkpoint/{net}/{dataset}/{i}',map_location=torch.device('cpu'))\n",
        "\n",
        "    fig, ax = plt.subplots(2,figsize=(15, 15))\n",
        "\n",
        "    for i in methods:\n",
        "      ax[0].plot(checkpoints[i]['train_losses'], label=i)\n",
        "    ax[0].set_title('train_losses')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_ylim([0, 1])\n",
        "\n",
        "    for i in methods:\n",
        "      ax[1].plot(checkpoints[i]['test_errs'], label=i)\n",
        "    ax[1].set_title('test_errs')\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].set_ylim([0, 0.2])\n",
        "\n",
        "    fig.suptitle(net)\n",
        "    plt.show()\n",
        "\n",
        "def plot_lrs(methods, dataset, net='vggnet'):\n",
        "    checkpoints={}\n",
        "    for i in methods:\n",
        "      checkpoints[i]=torch.load(f'{path}/checkpoint/{net}/{dataset}/{i}',map_location=torch.device('cpu'))\n",
        "\n",
        "    fig, ax = plt.subplots(1,figsize=(15, 15))\n",
        "\n",
        "    for i in methods:\n",
        "      ax.plot(checkpoints[i]['lrs'], label=i)\n",
        "    ax.set_title('lrs')\n",
        "    ax.legend()\n",
        "    ax.set_xlabel('Epochs')\n",
        "\n",
        "    fig.suptitle(net)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ppvx9g1uv8-x",
        "WBKTiu3-KSZs"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}